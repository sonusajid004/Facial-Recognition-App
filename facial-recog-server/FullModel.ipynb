{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imutils import paths\n",
    "import numpy as np\n",
    "import argparse\n",
    "import imutils\n",
    "import pickle\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imread(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading face detector...\n",
      "[INFO] loading face recognizer...\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading face detector...\")\n",
    "protoPath = \"./face_detection_model/deploy.prototxt\"\n",
    "modelPath = \"./face_detection_model/res10_300x300_ssd_iter_140000.caffemodel\"\n",
    "detector = cv2.dnn.readNetFromCaffe(protoPath, modelPath)\n",
    "# load our serialized face embedding model from disk\n",
    "print(\"[INFO] loading face recognizer...\")\n",
    "embedder = cv2.dnn.readNetFromTorch(\"./openface_nn4.small2.v1.t7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagePaths = list(paths.list_images(\"dataset\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize our lists of extracted facial embeddings and\n",
    "# corresponding people names\n",
    "knownEmbeddings = []\n",
    "knownNames = []\n",
    "# initialize the total number of faces processed\n",
    "total = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'imagePaths' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-25127efcee9b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimagePath\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimagePaths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;31m# extract the person name from the image path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     print(\"[INFO] processing image {}/{}\".format(i + 1,\n\u001b[0;32m      4\u001b[0m         len(imagePaths)))\n\u001b[0;32m      5\u001b[0m     \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimagePath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'imagePaths' is not defined"
     ]
    }
   ],
   "source": [
    "for (i, imagePath) in enumerate(imagePaths):\n",
    "    # extract the person name from the image path\n",
    "    print(\"[INFO] processing image {}/{}\".format(i + 1,\n",
    "        len(imagePaths)))\n",
    "    name = imagePath.split(os.path.sep)[-2]\n",
    "\n",
    "    # load the image, resize it to have a width of 600 pixels (while\n",
    "    # maintaining the aspect ratio), and then grab the image\n",
    "    # dimensions\n",
    "    image = cv2.imread(imagePath)\n",
    "    image = imutils.resize(image, width=600)\n",
    "    (h, w) = image.shape[:2]\n",
    "        # construct a blob from the image\n",
    "    imageBlob = cv2.dnn.blobFromImage(\n",
    "        cv2.resize(image, (300, 300)), 1.0, (300, 300),\n",
    "        (104.0, 177.0, 123.0), swapRB=False, crop=False)\n",
    "    # apply OpenCV's deep learning-based face detector to localize\n",
    "    # faces in the input image\n",
    "    detector.setInput(imageBlob)\n",
    "    detections = detector.forward()\n",
    "    # construct a blob from the image\n",
    "    imageBlob = cv2.dnn.blobFromImage(\n",
    "        cv2.resize(image, (300, 300)), 1.0, (300, 300),\n",
    "        (104.0, 177.0, 123.0), swapRB=False, crop=False)\n",
    "\n",
    "    # apply OpenCV's deep learning-based face detector to localize\n",
    "    # faces in the input image\n",
    "    detector.setInput(imageBlob)\n",
    "    detections = detector.forward()\n",
    "\n",
    "    # ensure at least one face was found\n",
    "    if len(detections) > 0:\n",
    "        # we're making the assumption that each image has only ONE\n",
    "        # face, so find the bounding box with the largest probability\n",
    "        i = np.argmax(detections[0, 0, :, 2])\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "\n",
    "        # ensure that the detection with the largest probability also\n",
    "        # means our minimum probability test (thus helping filter out\n",
    "        # weak detections)\n",
    "        if confidence > 0.75:\n",
    "            # compute the (x, y)-coordinates of the bounding box for\n",
    "            # the face\n",
    "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "            (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "\n",
    "            # extract the face ROI and grab the ROI dimensions\n",
    "            face = image[startY:endY, startX:endX]\n",
    "            (fH, fW) = face.shape[:2]\n",
    "\n",
    "            # ensure the face width and height are sufficiently large\n",
    "            if fW < 20 or fH < 20:\n",
    "                continue\n",
    "\n",
    "            # construct a blob for the face ROI, then pass the blob\n",
    "            # through our face embedding model to obtain the 128-d\n",
    "            # quantification of the face\n",
    "            faceBlob = cv2.dnn.blobFromImage(face, 1.0 / 255,\n",
    "                (96, 96), (0, 0, 0), swapRB=True, crop=False)\n",
    "            embedder.setInput(faceBlob)\n",
    "            vec = embedder.forward()\n",
    "\n",
    "            # add the name of the person + corresponding face\n",
    "            # embedding to their respective lists\n",
    "            knownNames.append(name)\n",
    "            knownEmbeddings.append(vec.flatten())\n",
    "            total += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] serializing 25 encodings...\n"
     ]
    }
   ],
   "source": [
    "# dump the facial embeddings + names to disk\n",
    "print(\"[INFO] serializing {} encodings...\".format(total))\n",
    "data = {\"embeddings\": knownEmbeddings, \"names\": knownNames}\n",
    "f = open(\"output/embeddings.pickle\", \"wb\")\n",
    "f.write(pickle.dumps(data))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import SVC\n",
    "import argparse\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading face embeddings...\n",
      "[INFO] encoding labels...\n"
     ]
    }
   ],
   "source": [
    "# load the face embeddings\n",
    "print(\"[INFO] loading face embeddings...\")\n",
    "data = pickle.loads(open(\"output/embeddings.pickle\", \"rb\").read())\n",
    "# encode the labels\n",
    "print(\"[INFO] encoding labels...\")\n",
    "le = LabelEncoder()\n",
    "labels = le.fit_transform(data[\"names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training model...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
       "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
       "    verbose=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model used to accept the 128-d embeddings of the face and\n",
    "# then produce the actual face recognition\n",
    "print(\"[INFO] training model...\")\n",
    "recognizer = SVC(C=1.0, kernel=\"linear\", probability=True)\n",
    "recognizer.fit(data[\"embeddings\"], labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the actual face recognition model to disk\n",
    "f = open(\"output/recognizer.pickle\", \"wb\")\n",
    "f.write(pickle.dumps(recognizer))\n",
    "f.close()\n",
    "# write the label encoder to disk\n",
    "f = open(\"output/le.pickle\", \"wb\")\n",
    "f.write(pickle.dumps(le))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imutils import paths\n",
    "import numpy as np\n",
    "import argparse\n",
    "import imutils\n",
    "import pickle\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading face detector...\n",
      "[INFO] loading face recognizer...\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading face detector...\")\n",
    "protoPath = \"./face_detection_model/deploy.prototxt\"\n",
    "modelPath = \"./face_detection_model/res10_300x300_ssd_iter_140000.caffemodel\"\n",
    "detector = cv2.dnn.readNetFromCaffe(protoPath, modelPath)\n",
    "# load our serialized face embedding model from disk\n",
    "print(\"[INFO] loading face recognizer...\")\n",
    "embedder = cv2.dnn.readNetFromTorch(\"./openface_nn4.small2.v1.t7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the actual face recognition model along with the label encoder\n",
    "recognizer = pickle.loads(open(\"output/recognizer.pickle\", \"rb\").read())\n",
    "le = pickle.loads(open(\"output/le.pickle\", \"rb\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['images/sajid\\\\13.jpeg',\n",
       " 'images/sajid\\\\14.jpeg',\n",
       " 'images/sajid\\\\2.jpeg',\n",
       " 'images/sajid\\\\4.jpeg']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(paths.list_images(\"images/sajid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the image, resize it to have a width of 600 pixels (while\n",
    "# maintaining the aspect ratio), and then grab the image dimensions\n",
    "image = cv2.imread(\"images/sajid/2.jpeg\")\n",
    "cv2.imshow(\"Image\",image)\n",
    "image = imutils.resize(image, width=600)\n",
    "(h, w) = image.shape[:2]\n",
    "# construct a blob from the image\n",
    "imageBlob = cv2.dnn.blobFromImage(\n",
    "\tcv2.resize(image, (300, 300)), 1.0, (300, 300),\n",
    "\t(104.0, 177.0, 123.0), swapRB=False, crop=False)\n",
    "# apply OpenCV's deep learning-based face detector to localize\n",
    "# faces in the input image\n",
    "detector.setInput(imageBlob)\n",
    "detections = detector.forward()\n",
    "\n",
    "# loop over the detections\n",
    "for i in range(0, detections.shape[2]):\n",
    "\t# extract the confidence (i.e., probability) associated with the\n",
    "\t# prediction\n",
    "\tconfidence = detections[0, 0, i, 2]\n",
    "\n",
    "\tif confidence > 0.75:\n",
    "\t\t# compute the (x, y)-coordinates of the bounding box for the\n",
    "\t\t\n",
    "        \n",
    "\t\tbox = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "\t\t(startX, startY, endX, endY) = box.astype(\"int\")\n",
    "\t\t# extract the face ROI\n",
    "\t\tface = image[startY:endY, startX:endX]\n",
    "\t\t(fH, fW) = face.shape[:2]\n",
    "\t\t# ensure the face width and height are sufficiently large\n",
    "\t\tif fW < 20 or fH < 20:\n",
    "\t\t\tcontinue\n",
    "            \n",
    "\t\tfaceBlob = cv2.dnn.blobFromImage(face, 1.0 / 255, (96, 96),\n",
    "\t\t\t(0, 0, 0), swapRB=True, crop=False)\n",
    "\t\tembedder.setInput(faceBlob)\n",
    "\t\tvec = embedder.forward()\n",
    "\t\t# perform classification to recognize the face\n",
    "\t\tpreds = recognizer.predict_proba(vec)[0]\n",
    "\t\tj = np.argmax(preds)\n",
    "\t\tproba = preds[j]\n",
    "\t\tname = le.classes_[j]\n",
    "        # draw the bounding box of the face along with the associated\n",
    "\t\t# probability\n",
    "\t\ttext = \"{}: {:.2f}%\".format(name, proba * 100)\n",
    "\t\ty = startY - 10 if startY - 10 > 10 else startY + 10\n",
    "\t\tcv2.rectangle(image, (startX, startY), (endX, endY),\n",
    "\t\t\t(0, 0, 255), 2)\n",
    "\t\tcv2.putText(image, text, (startX, y),\n",
    "\t\t\tcv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 255), 2)\n",
    "# show the output image\n",
    "cv2.imshow(\"Image\", image)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from imutils.video import VideoStream\n",
    "from imutils.video import FPS\n",
    "import numpy as np\n",
    "import argparse\n",
    "import imutils\n",
    "import pickle\n",
    "import time\n",
    "import cv2\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] starting video stream...\n",
      "[INFO] elasped time: 67.29\n",
      "[INFO] approx. FPS: 11.15\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# initialize the video stream, then allow the camera sensor to warm up\n",
    "print(\"[INFO] starting video stream...\")\n",
    "vs = VideoStream(src=0).start()\n",
    "time.sleep(2.0)\n",
    "\n",
    "# start the FPS throughput estimator\n",
    "fps = FPS().start()\n",
    "\n",
    "# loop over frames from the video file stream\n",
    "while True:\n",
    "\t# grab the frame from the threaded video stream\n",
    "\tframe = vs.read()\n",
    "\n",
    "\t# resize the frame to have a width of 600 pixels (while\n",
    "\t# maintaining the aspect ratio), and then grab the image\n",
    "\t# dimensions\n",
    "\tframe = imutils.resize(frame, width=600)\n",
    "\t(h, w) = frame.shape[:2]\n",
    "\n",
    "\t# construct a blob from the image\n",
    "\timageBlob = cv2.dnn.blobFromImage(\n",
    "\t\tcv2.resize(frame, (300, 300)), 1.0, (300, 300),\n",
    "\t\t(104.0, 177.0, 123.0), swapRB=False, crop=False)\n",
    "\n",
    "\t# apply OpenCV's deep learning-based face detector to localize\n",
    "\t# faces in the input image\n",
    "\tdetector.setInput(imageBlob)\n",
    "\tdetections = detector.forward()\n",
    "\n",
    "\t# loop over the detections\n",
    "\tfor i in range(0, detections.shape[2]):\n",
    "\t\t# extract the confidence (i.e., probability) associated with\n",
    "\t\t# the prediction\n",
    "\t\tconfidence = detections[0, 0, i, 2]\n",
    "\n",
    "\t\t# filter out weak detections\n",
    "\t\tif confidence > 0.5:\n",
    "\t\t\t# compute the (x, y)-coordinates of the bounding box for\n",
    "\t\t\t# the face\n",
    "\t\t\tbox = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "\t\t\t(startX, startY, endX, endY) = box.astype(\"int\")\n",
    "\n",
    "\t\t\t# extract the face ROI\n",
    "\t\t\tface = frame[startY:endY, startX:endX]\n",
    "\t\t\t(fH, fW) = face.shape[:2]\n",
    "\n",
    "\t\t\t# ensure the face width and height are sufficiently large\n",
    "\t\t\tif fW < 20 or fH < 20:\n",
    "\t\t\t\tcontinue\n",
    "\n",
    "\t\t\t# construct a blob for the face ROI, then pass the blob\n",
    "\t\t\t# through our face embedding model to obtain the 128-d\n",
    "\t\t\t# quantification of the face\n",
    "\t\t\tfaceBlob = cv2.dnn.blobFromImage(face, 1.0 / 255,\n",
    "\t\t\t\t(96, 96), (0, 0, 0), swapRB=True, crop=False)\n",
    "\t\t\tembedder.setInput(faceBlob)\n",
    "\t\t\tvec = embedder.forward()\n",
    "\n",
    "\t\t\t# perform classification to recognize the face\n",
    "\t\t\tpreds = recognizer.predict_proba(vec)[0]\n",
    "\t\t\tj = np.argmax(preds)\n",
    "\t\t\tproba = preds[j]\n",
    "\t\t\tname = le.classes_[j]\n",
    "\n",
    "\t\t\t# draw the bounding box of the face along with the\n",
    "\t\t\t# associated probability\n",
    "\t\t\ttext = \"{}: {:.2f}%\".format(name, proba * 100)\n",
    "\t\t\ty = startY - 10 if startY - 10 > 10 else startY + 10\n",
    "\t\t\tcv2.rectangle(frame, (startX, startY), (endX, endY),\n",
    "\t\t\t\t(0, 0, 255), 2)\n",
    "\t\t\tcv2.putText(frame, text, (startX, y),\n",
    "\t\t\t\tcv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 255), 2)\n",
    "\n",
    "\t# update the FPS counter\n",
    "\tfps.update()\n",
    "\n",
    "\t# show the output frame\n",
    "\tcv2.imshow(\"Frame\", frame)\n",
    "\tkey = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "\t# if the `q` key was pressed, break from the loop\n",
    "\tif key == ord(\"q\"):\n",
    "\t\tbreak\n",
    "\n",
    "# stop the timer and display FPS information\n",
    "fps.stop()\n",
    "print(\"[INFO] elasped time: {:.2f}\".format(fps.elapsed()))\n",
    "print(\"[INFO] approx. FPS: {:.2f}\".format(fps.fps()))\n",
    "\n",
    "# do a bit of cleanup\n",
    "cv2.destroyAllWindows()\n",
    "vs.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import request\n",
    "from flask import jsonify\n",
    "from flask_cors import CORS, cross_origin\n",
    "from flask import render_template\n",
    "from flask import Flask\n",
    "import base64\n",
    "from PIL import Image\n",
    "import io\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object list_files at 0x0000013260B78A48>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from os import path\n",
    "path.exists(\"dataset/munna\")\n",
    "# simple version for working with CWD\n",
    "paths.list_images(\"datset/sajid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from flask import request\n",
    "from flask import jsonify\n",
    "from flask_cors import CORS, cross_origin\n",
    "from flask import render_template\n",
    "from flask import Flask\n",
    "import base64\n",
    "from PIL import Image\n",
    "import io\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imutils import paths\n",
    "import numpy as np\n",
    "import argparse\n",
    "import imutils\n",
    "import pickle\n",
    "import cv2\n",
    "import os\n",
    "from os import path\n",
    "app = Flask(__name__)\n",
    "deco=None\n",
    "msg = None\n",
    "\n",
    "\n",
    "print(\"[INFO] loading face detector...\")\n",
    "protoPath = \"./face_detection_model/deploy.prototxt\"\n",
    "modelPath = \"./face_detection_model/res10_300x300_ssd_iter_140000.caffemodel\"\n",
    "detector = cv2.dnn.readNetFromCaffe(protoPath, modelPath)\n",
    "# load our serialized face embedding model from disk\n",
    "print(\"[INFO] loading face recognizer...\")\n",
    "embedder = cv2.dnn.readNetFromTorch(\"./openface_nn4.small2.v1.t7\")\n",
    "\n",
    "\n",
    "\n",
    "@app.route(\"/train\", methods=[\"POST\"])\n",
    "@cross_origin(origin='localhost',headers=['Content- Type','Authorization'])\n",
    "def train():\n",
    "    print(\"Train started\")\n",
    "    # load the face embeddings\n",
    "    print(\"[INFO] loading face embeddings...\")\n",
    "    data = pickle.loads(open(\"output/embeddings.pickle\", \"rb\").read())\n",
    "   \n",
    "    message = request.get_json(force=True)\n",
    "   \n",
    "    name = message['name']\n",
    "    print(message['name'])\n",
    "    encoded = message['image']\n",
    "    for i in encoded:\n",
    "        decoded = base64.b64decode(i)\n",
    "        image = Image.open(io.BytesIO(decoded))\n",
    "        path_name=\"dataset/\"+name\n",
    "        if(not path.exists(\"dataset/\"+name)):\n",
    "            os.mkdir(\"dataset/\"+name)\n",
    "            image.save(\"dataset/\"+name+\"/id_\"+str(0)+\".png\")\n",
    "        else:\n",
    "            length = len(list(paths.list_images(path_name)))\n",
    "            image.save(\"dataset/\"+name+\"/id_\"+str(length+1)+\".png\")\n",
    "\n",
    "        image = np.array(image)\n",
    "\n",
    "        frame = imutils.resize(image, width=600)\n",
    "        (h, w) = frame.shape[:2]\n",
    "        # construct a blob from the image\n",
    "        imageBlob = cv2.dnn.blobFromImage(\n",
    "            cv2.resize(frame, (300, 300)), 1.0, (300, 300),\n",
    "            (104.0, 177.0, 123.0), swapRB=False, crop=False)\n",
    "\n",
    "        # apply OpenCV's deep learning-based face detector to localize\n",
    "        # faces in the input image\n",
    "        detector.setInput(imageBlob)\n",
    "        detections = detector.forward()\n",
    "\n",
    "        i = np.argmax(detections[0, 0, :, 2])\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "        # filter out weak detections\n",
    "        if confidence > 0.5:\n",
    "            # compute the (x, y)-coordinates of the bounding box for\n",
    "            # the face\n",
    "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "            (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "\n",
    "            # extract the face ROI\n",
    "            face = frame[startY:endY, startX:endX]\n",
    "            (fH, fW) = face.shape[:2]\n",
    "            # construct a blob for the face ROI, then pass the blob\n",
    "            # through our face embedding model to obtain the 128-d\n",
    "            # quantification of the face\n",
    "            faceBlob = cv2.dnn.blobFromImage(face, 1.0 / 255,\n",
    "                (96, 96), (0, 0, 0), swapRB=True, crop=False)\n",
    "            embedder.setInput(faceBlob)\n",
    "            vec = embedder.forward()\n",
    "\n",
    "            data['embeddings'].append(vec.flatten())\n",
    "            data['names'].append(name)\n",
    "    # train the model used to accept the 128-d embeddings of the face and\n",
    "    # then produce the actual face recognition\n",
    "    le = LabelEncoder()\n",
    "    labels = le.fit_transform(data['names'])\n",
    "    print(\"[INFO] training model SVC with new data...\")\n",
    "    #training SVC with new Data\n",
    "    recognizer = SVC(C=1.0, kernel=\"linear\", probability=True)\n",
    "    recognizer.fit(data[\"embeddings\"], labels)\n",
    "    # dump the facial embeddings + names to disk\n",
    "    data = {\"embeddings\": data['embeddings'], \"names\": data['names']}\n",
    "    f = open(\"output/embeddings.pickle\", \"wb\")\n",
    "    f.write(pickle.dumps(data))\n",
    "    f.close()\n",
    "    # write the actual face recognition model to disk\n",
    "    f = open(\"output/recognizer.pickle\", \"wb\")\n",
    "    f.write(pickle.dumps(recognizer))\n",
    "    f.close()\n",
    "    # write the label encoder to disk\n",
    "    f = open(\"output/le.pickle\", \"wb\")\n",
    "    f.write(pickle.dumps(le))\n",
    "    f.close()\n",
    "    print(\"Ceompleted Process\")\n",
    "    return jsonify({'status':True})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@app.route(\"/predict\", methods=[\"POST\"])\n",
    "@cross_origin(origin='localhost',headers=['Content- Type','Authorization'])\n",
    "def predict():\n",
    "    # load the actual face recognition model along with the label encoder\n",
    "    recognizer = pickle.loads(open(\"output/recognizer.pickle\", \"rb\").read())\n",
    "    le = pickle.loads(open(\"output/le.pickle\", \"rb\").read())\n",
    "    print(\"hello started\")\n",
    "    message = request.get_json(force=True)\n",
    "    encoded = message['image']\n",
    "    decoded = base64.b64decode(encoded)\n",
    "    global deco\n",
    "    deco = decoded\n",
    "    image = Image.open(io.BytesIO(decoded))\n",
    "   \n",
    "    image = np.array(image)\n",
    "    frame = imutils.resize(image, width=600)\n",
    "    (h, w) = frame.shape[:2]\n",
    "    # construct a blob from the image\n",
    "    imageBlob = cv2.dnn.blobFromImage(\n",
    "        cv2.resize(frame, (300, 300)), 1.0, (300, 300),\n",
    "        (104.0, 177.0, 123.0), swapRB=False, crop=False)\n",
    "\n",
    "    # apply OpenCV's deep learning-based face detector to localize\n",
    "    # faces in the input image\n",
    "    detector.setInput(imageBlob)\n",
    "    detections = detector.forward()\n",
    "\n",
    "    i = np.argmax(detections[0, 0, :, 2])\n",
    "    confidence = detections[0, 0, i, 2]\n",
    "    # filter out weak detections\n",
    "    if confidence > 0.5:\n",
    "        # compute the (x, y)-coordinates of the bounding box for\n",
    "        # the face\n",
    "        box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "        (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "\n",
    "        # extract the face ROI\n",
    "        face = frame[startY:endY, startX:endX]\n",
    "        (fH, fW) = face.shape[:2]\n",
    "        # construct a blob for the face ROI, then pass the blob\n",
    "        # through our face embedding model to obtain the 128-d\n",
    "        # quantification of the face\n",
    "        faceBlob = cv2.dnn.blobFromImage(face, 1.0 / 255,\n",
    "            (96, 96), (0, 0, 0), swapRB=True, crop=False)\n",
    "        embedder.setInput(faceBlob)\n",
    "        vec = embedder.forward()\n",
    "\n",
    "        # perform classification to recognize the face\n",
    "        preds = recognizer.predict_proba(vec)[0]\n",
    "        j = np.argmax(preds)\n",
    "        proba = preds[j]\n",
    "        name = le.classes_[j]\n",
    "        response = {\n",
    "            'prediction': {\n",
    "             'name': name,\n",
    "                'prob': proba\n",
    "            }\n",
    "        }\n",
    "        print(response)\n",
    "        return jsonify(response)\n",
    "    else:\n",
    "        return jsonify({'name':'error'})\n",
    "\n",
    "@app.route(\"/\")\n",
    "def hello():\n",
    "    return jsonify({'name':'sajid'})\n",
    "    \n",
    "app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.22.1'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
